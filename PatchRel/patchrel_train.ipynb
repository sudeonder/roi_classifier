{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e948a33",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8e948a33"
      },
      "outputs": [],
      "source": [
        "# patchrel training (without ROI augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "ZQviZ8AYT6_f",
        "outputId": "807622a8-d473-401c-d4bd-c15227ea1c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZQviZ8AYT6_f",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PatchRel (two-branch patch classifier with ROI-level relevance aggregation)\n",
        "First steps — NO ROIAugment yet (flag is wired but disabled by default).\n",
        "Directory layout (expected):\n",
        "  MyDrive/BRACS/ROIPatches/{split}/{lesion}/{ROI_name}/*.png\n",
        "Where {lesion} ∈ {N, PB, UDH, FEA, ADH, DCIS, IC} (case-insensitive tolerated).\n",
        "We map these 7 to 3 superclasses: B (Benign), A (Atypical), M (Malignant).\n",
        "\n",
        "This file provides:\n",
        "  - Config dataclass\n",
        "  - Lesion→Superclass mapping utilities\n",
        "  - ROI Patch Dataset with clean sanity checks\n",
        "  - FixedPatchBatchSampler (caps patches/ROI and total patches/batch)\n",
        "  - Collate fn with optional UNION augmentation hook (disabled)\n",
        "  - PatchRel model (EfficientNet-B0 backbone via timm)\n",
        "  - One quick dry-run sanity function\n",
        "\n",
        "Requirements: timm, torch, torchvision, pillow\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Sequence\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from PIL import Image\n",
        "\n",
        "try:\n",
        "    import timm  # type: ignore\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Please install timm: pip install timm\") from e\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    drive_root: Path = Path(\"/content/drive/MyDrive/BRACS/ROIPatches\")\n",
        "    split: str = \"train\"  # {train,val,test}\n",
        "    image_size: int = 224\n",
        "    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
        "    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
        "\n",
        "    # I/O speed knobs\n",
        "    fast_io_cap: Optional[int] = None  # if set, __getitem__ will load at most this many patches per ROI\n",
        "\n",
        "    # batching\n",
        "    batch_total_patches: int = 512\n",
        "    max_patches_per_roi: int = 30\n",
        "\n",
        "    # dataloader\n",
        "    num_workers: int = 2\n",
        "    pin_memory: bool = True\n",
        "    persistent_workers: bool = False\n",
        "\n",
        "    # oversampling of superclasses at ROI level\n",
        "    oversample_multiplier: Dict[str, int] = None  # set in __post_init__\n",
        "\n",
        "    # augmentation flags (ROIAugment not used here yet)\n",
        "    enable_roi_augment: bool = False\n",
        "    roi_augment_prob: float = 0.25\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.oversample_multiplier is None:\n",
        "\n",
        "            self.oversample_multiplier = {\"B\": 1, \"A\": 1, \"M\": 1}\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Label mapping utilities\n",
        "# -------------------------------\n",
        "SUPERCLASSES = [\"B\", \"A\", \"M\"]\n",
        "SUPERCLASS_TO_IDX = {c: i for i, c in enumerate(SUPERCLASSES)}\n",
        "\n",
        "# Map 7 lesion folders into 3 superclasses\n",
        "LESION_TO_SUPER = {\n",
        "    \"N\": \"B\", \"PB\": \"B\", \"UDH\": \"B\",\n",
        "    \"FEA\": \"A\", \"ADH\": \"A\",\n",
        "    \"DCIS\": \"M\", \"IC\": \"M\",\n",
        "}\n",
        "\n",
        "\n",
        "def infer_superclass_from_lesion_folder(name: str) -> str:\n",
        "    \"\"\"Robustly infer superclass from a lesion folder name.\n",
        "    Accepts names like 'N', 'PB', 'UDH', 'FEA', 'ADH', 'DCIS', 'IC', and tolerant variants\n",
        "    such as '0_N', '1-PB', 'Type_UDH', etc. Strategy: split by non-letters and look for a\n",
        "    token that matches one of the 7 canonical lesion codes.\n",
        "    \"\"\"\n",
        "    tokens = [t for t in re.split(r\"[^A-Za-z]+\", name.upper()) if t]\n",
        "    for t in tokens:\n",
        "        if t in LESION_TO_SUPER:\n",
        "            return LESION_TO_SUPER[t]\n",
        "    raise ValueError(\n",
        "        f\"Unrecognized lesion folder '{name}'. Expected one of {list(LESION_TO_SUPER)} or tolerant variants like '0_N'.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Light transforms (TorchVision-free to keep deps small)\n",
        "# -------------------------------\n",
        "class SimpleResizeNormalize:\n",
        "    def __init__(self, size: int, mean: Tuple[float, float, float], std: Tuple[float, float, float]):\n",
        "        self.size = size\n",
        "        self.mean = torch.tensor(mean).view(3, 1, 1)\n",
        "        self.std = torch.tensor(std).view(3, 1, 1)\n",
        "\n",
        "    def __call__(self, pil_img: Image.Image) -> torch.Tensor:\n",
        "        # Fast, warning-free: use numpy instead of Torch ByteStorage path\n",
        "        img = pil_img.convert(\"RGB\").resize((self.size, self.size), resample=Image.BILINEAR)\n",
        "        import numpy as np\n",
        "        arr = np.asarray(img, dtype=np.float32) / 255.0  # [H,W,3]\n",
        "        t = torch.from_numpy(arr).permute(2, 0, 1)       # [3,H,W]\n",
        "        return (t - self.mean) / self.std\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# ROI Patch Dataset\n",
        "# -------------------------------\n",
        "COORD_PAT = re.compile(r\"(?:x|X)(-?\\d+)[^\\d]+(?:y|Y)(-?\\d+)\")\n",
        "\n",
        "\n",
        "class ROIPatchDataset(Dataset):\n",
        "    \"\"\"Walks the Drive directory and builds an index of (roi_dir, class_idx, list_of_patch_paths).\n",
        "       Each __getitem__ returns the patches for a *single ROI* lazily (the sampler will pick ROI ids).\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.root = cfg.drive_root / cfg.split\n",
        "        if not self.root.exists():\n",
        "            raise FileNotFoundError(f\"Split folder not found: {self.root}\")\n",
        "        self.transform = SimpleResizeNormalize(cfg.image_size, cfg.normalize_mean, cfg.normalize_std)\n",
        "\n",
        "        self.roi_items: List[Tuple[Path, int, List[Path]]] = []  # (roi_dir, superclass_idx, patch_list)\n",
        "        self._build_index()\n",
        "\n",
        "        # Class index list for oversampling decisions\n",
        "        self.roi_super: List[int] = [it[1] for it in self.roi_items]\n",
        "\n",
        "    def _build_index(self):\n",
        "        lesion_folders = [p for p in self.root.iterdir() if p.is_dir()]\n",
        "        total_patches = 0\n",
        "        for lf in sorted(lesion_folders):\n",
        "            super_c = infer_superclass_from_lesion_folder(lf.name)\n",
        "            c_idx = SUPERCLASS_TO_IDX[super_c]\n",
        "            for roi_dir in sorted([d for d in lf.iterdir() if d.is_dir()]):\n",
        "                patches = sorted([p for p in roi_dir.glob(\"*.png\")])\n",
        "                if len(patches) < 5:\n",
        "                    continue  # drop tiny ROIs as per paper\n",
        "                self.roi_items.append((roi_dir, c_idx, patches))\n",
        "                total_patches += len(patches)\n",
        "\n",
        "        # --- Sanity checks ---\n",
        "        assert len(self.roi_items) > 0, f\"No ROIs found under {self.root}\"\n",
        "        counts = {\"B\": 0, \"A\": 0, \"M\": 0}\n",
        "        for _, c_idx, _ in self.roi_items:\n",
        "            counts[SUPERCLASSES[c_idx]] += 1\n",
        "        print(f\"[Index] Split={self.cfg.split} ROIs={len(self.roi_items)} per-class={counts} (patches~{total_patches})\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.roi_items)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        roi_dir, c_idx, patch_paths = self.roi_items[idx]\n",
        "        # Optional early cap at dataset level to avoid loading all files (helps with slow I/O)\n",
        "        load_paths = patch_paths\n",
        "        if self.cfg.fast_io_cap is not None and len(load_paths) > self.cfg.fast_io_cap:\n",
        "            load_paths = load_paths[: self.cfg.fast_io_cap]\n",
        "        images: List[torch.Tensor] = []\n",
        "        coords: List[Tuple[int, int]] = []\n",
        "        for p in load_paths:\n",
        "            try:\n",
        "                img = Image.open(p)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Failed to open patch: {p}\") from e\n",
        "            images.append(self.transform(img))\n",
        "            m = COORD_PAT.search(p.name)\n",
        "            if m:\n",
        "                coords.append((int(m.group(1)), int(m.group(2))))\n",
        "            else:\n",
        "                coords.append((0, 0))\n",
        "        images_t = torch.stack(images, dim=0)  # [L, 3, H, W]\n",
        "        coords_t = torch.tensor(coords, dtype=torch.long)  # [L, 2]\n",
        "        target = torch.tensor(c_idx, dtype=torch.long)\n",
        "        return {\n",
        "            \"roi_id\": idx,\n",
        "            \"roi_dir\": str(roi_dir),\n",
        "            \"images\": images_t,\n",
        "            \"coords\": coords_t,\n",
        "            \"target\": target,\n",
        "        }\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Sampler + Collate\n",
        "# -------------------------------\n",
        "class FixedPatchBatchSampler(Sampler[List[int]]):\n",
        "    \"\"\"Yields lists of ROI indices so that the *total* patches in the subsequent collate\n",
        "    will be ≈ cfg.batch_total_patches, with per-ROI cap cfg.max_patches_per_roi.\n",
        "    Implements simple oversampling by repeating ROI indices based on superclass multipliers.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: ROIPatchDataset, cfg: Config, steps_per_epoch: int = 500):\n",
        "        self.ds = dataset\n",
        "        self.cfg = cfg\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.roi_indices_by_super = {0: [], 1: [], 2: []}\n",
        "        for i, c in enumerate(self.ds.roi_super):\n",
        "            self.roi_indices_by_super[c].append(i)\n",
        "        # Simple safety\n",
        "        for k, v in self.roi_indices_by_super.items():\n",
        "            if len(v) == 0:\n",
        "                print(f\"[WARN] No ROIs for class {SUPERCLASSES[k]} in split {cfg.split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __iter__(self):\n",
        "        cfg = self.cfg\n",
        "        while True:\n",
        "            # Build a pool with oversampling\n",
        "            pool: List[int] = []\n",
        "            for super_c, idxs in self.roi_indices_by_super.items():\n",
        "                mult = cfg.oversample_multiplier[SUPERCLASSES[super_c]]\n",
        "                if len(idxs) > 0 and mult > 0:\n",
        "                    pool.extend(random.choices(idxs, k=max(mult * 4, mult)))\n",
        "            if not pool:  # fallback\n",
        "                pool = list(range(len(self.ds)))\n",
        "            random.shuffle(pool)\n",
        "\n",
        "            batch_roi_idxs: List[int] = []\n",
        "            patch_budget = cfg.batch_total_patches\n",
        "            for roi_idx in pool:\n",
        "                L = self._roi_length_capped(roi_idx)\n",
        "                if L <= 0:\n",
        "                    continue\n",
        "                if L > patch_budget:\n",
        "                    if len(batch_roi_idxs) == 0:  # ensure we always yield something\n",
        "                        batch_roi_idxs.append(roi_idx)\n",
        "                    break\n",
        "                batch_roi_idxs.append(roi_idx)\n",
        "                patch_budget -= L\n",
        "                if patch_budget <= 0:\n",
        "                    break\n",
        "\n",
        "            yield batch_roi_idxs\n",
        "\n",
        "    def _roi_length_capped(self, roi_idx: int) -> int:\n",
        "        L = self.ds.roi_items[roi_idx][2]\n",
        "        return min(len(L), self.cfg.max_patches_per_roi)\n",
        "\n",
        "\n",
        "def collate_roi_batch(samples: List[dict], cfg: Config) -> dict:\n",
        "    \"\"\"Convert a list of ROI samples into a flat patch batch with grouping information.\n",
        "       ROIAugment hook is wired but disabled by default.\n",
        "    \"\"\"\n",
        "    # Optionally, ROIAugment would go here (disabled now)\n",
        "\n",
        "    images_list: List[torch.Tensor] = []\n",
        "    coords_list: List[torch.Tensor] = []\n",
        "    roi_ids: List[int] = []  # per-patch roi group id within the batch\n",
        "    targets: List[int] = []  # per-ROI class index (single-label for now)\n",
        "\n",
        "    group_id = 0\n",
        "    for s in samples:\n",
        "        img = s[\"images\"]\n",
        "        coords = s[\"coords\"]\n",
        "        # Cap per ROI\n",
        "        if img.size(0) > cfg.max_patches_per_roi:\n",
        "            # deterministic crop for reproducibility (can switch to random later)\n",
        "            img = img[: cfg.max_patches_per_roi]\n",
        "            coords = coords[: cfg.max_patches_per_roi]\n",
        "        L = img.size(0)\n",
        "        images_list.append(img)\n",
        "        coords_list.append(coords)\n",
        "        roi_ids.extend([group_id] * L)\n",
        "        targets.append(int(s[\"target\"]))\n",
        "        group_id += 1\n",
        "\n",
        "    images = torch.cat(images_list, dim=0)  # [P,3,H,W]\n",
        "    coords = torch.cat(coords_list, dim=0)  # [P,2]\n",
        "    roi_ids_t = torch.tensor(roi_ids, dtype=torch.long)  # [P]\n",
        "    roi_targets = torch.tensor(targets, dtype=torch.long)  # [R]\n",
        "\n",
        "    # --- Sanity checks ---\n",
        "    assert images.ndim == 4 and images.size(1) == 3\n",
        "    assert coords.size(0) == images.size(0)\n",
        "    assert roi_ids_t.size(0) == images.size(0)\n",
        "    assert roi_targets.size(0) == group_id\n",
        "\n",
        "    return {\n",
        "        \"images\": images,\n",
        "        \"coords\": coords,\n",
        "        \"roi_ids\": roi_ids_t,\n",
        "        \"roi_targets\": roi_targets,  # int labels 0..2 (B,A,M)\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# PatchRel Model (no ROI augmentation required here)\n",
        "# -------------------------------\n",
        "class PatchRel(nn.Module):\n",
        "    def __init__(self, num_classes: int = 3, drop_rate: float = 0.1, drop_path_rate: float = 0.25, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            \"efficientnet_b0\", pretrained=pretrained, num_classes=0, drop_rate=drop_rate, drop_path_rate=drop_path_rate\n",
        "        )\n",
        "        in_feats = getattr(self.backbone, \"num_features\", 1280)\n",
        "        self.classifier = nn.Linear(in_feats, num_classes)  # ω branch: per-patch class logits\n",
        "        self.relevance = nn.Linear(in_feats, num_classes)   # β branch: per-class, per-patch relevance logits\n",
        "\n",
        "    @staticmethod\n",
        "    def _group_softmax(logits: torch.Tensor, roi_ids: torch.Tensor, num_rois: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Softmax over patches within each ROI, per class.\n",
        "        logits: [P, C], roi_ids: [P] in {0..R-1} -> alpha: [P, C] with sum_j alpha_{ij}^c = 1.\n",
        "        \"\"\"\n",
        "        P, C = logits.shape\n",
        "\n",
        "        # max per (ROI, class) in original dtype (fp16 under autocast)\n",
        "        max_per_roi_class = torch.full((num_rois, C), -float(\"inf\"),\n",
        "                                      device=logits.device, dtype=logits.dtype)\n",
        "        max_per_roi_class.index_reduce_(0, roi_ids, logits, reduce=\"amax\")  # [R, C]\n",
        "        centered = logits - max_per_roi_class[roi_ids]  # [P, C]\n",
        "\n",
        "        # --- do exp + accumulation in float32 to avoid dtype clash & improve stability ---\n",
        "        centered32 = centered.float()\n",
        "        exp32 = centered32.exp()  # [P, C] float32\n",
        "\n",
        "        sum_per_roi_class32 = torch.zeros((num_rois, C), device=logits.device, dtype=torch.float32)\n",
        "        sum_per_roi_class32.index_add_(0, roi_ids, exp32)  # [R, C] float32\n",
        "\n",
        "        alpha32 = exp32 / (sum_per_roi_class32[roi_ids] + 1e-12)  # [P, C] float32\n",
        "        # -------------------------------------------------------------------------------\n",
        "\n",
        "        return alpha32.to(logits.dtype)  # cast α back to fp16 under autocast\n",
        "\n",
        "\n",
        "    def forward(self, images: torch.Tensor, roi_ids: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
        "        \"\"\"\n",
        "        images: [P,3,H,W], roi_ids: [P] (0..R-1)\n",
        "        Returns:\n",
        "          roi_scores: [R, C] (pre-sigmoid) — relevance-weighted sum of per-patch class logits→prob\n",
        "          aux dict: with patch-level outputs and α\n",
        "        \"\"\"\n",
        "        feats = self.backbone(images)  # [P, D]\n",
        "        patch_logits = self.classifier(feats)  # [P, C]\n",
        "        patch_probs = patch_logits.softmax(dim=1)  # per-patch class likelihoods p(ŷ=c|x)\n",
        "\n",
        "        rel_logits = self.relevance(feats)  # [P, C]\n",
        "        num_rois = int(roi_ids.max().item()) + 1 if roi_ids.numel() > 0 else 0\n",
        "        alpha = self._group_softmax(rel_logits, roi_ids, num_rois)  # [P, C]\n",
        "\n",
        "        # Aggregate to ROI level: p(y^c|R) = sum_j α_jc * p(ŷ_j=c|x_j)\n",
        "        weighted = alpha * patch_probs  # [P, C]\n",
        "        roi_scores = torch.zeros((num_rois, patch_logits.size(1)), device=images.device, dtype=weighted.dtype)\n",
        "        roi_scores.index_add_(0, roi_ids, weighted)  # [R, C]\n",
        "\n",
        "        # For loss we prefer logits; convert ROI probs→logits with clamp\n",
        "        roi_probs = roi_scores.clamp(1e-6, 1 - 1e-6)\n",
        "        roi_logits = torch.log(roi_probs) - torch.log1p(-roi_probs)\n",
        "\n",
        "        aux = {\n",
        "            \"patch_logits\": patch_logits,\n",
        "            \"patch_probs\": patch_probs,\n",
        "            \"rel_logits\": rel_logits,\n",
        "            \"alpha\": alpha,\n",
        "            \"roi_probs\": roi_probs,\n",
        "        }\n",
        "        return roi_logits, aux\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Loss & simple metrics\n",
        "# -------------------------------\n",
        "class ROIBCELoss(nn.Module):\n",
        "    def __init__(self, num_classes: int = 3):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, roi_logits: torch.Tensor, roi_targets: torch.Tensor) -> torch.Tensor:\n",
        "        # roi_targets are single-label ints 0..C-1 → convert to one-hot\n",
        "        one_hot = torch.zeros((roi_targets.size(0), self.num_classes), device=roi_logits.device, dtype=roi_logits.dtype)\n",
        "        one_hot.scatter_(1, roi_targets.view(-1, 1), 1.0)\n",
        "        return self.bce(roi_logits, one_hot)\n",
        "\n",
        "\n",
        "def gmean_from_confusion(conf: torch.Tensor) -> float:\n",
        "    \"\"\"conf: [C,C] where conf[t,p] counts ROIs of true t predicted p.\n",
        "       Returns geometric mean of per-class recalls.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        recalls = []\n",
        "        for c in range(conf.size(0)):\n",
        "            denom = conf[c].sum().item()\n",
        "            rec = (conf[c, c].item() / denom) if denom > 0 else 0.0\n",
        "            recalls.append(max(rec, 1e-12))\n",
        "        prod = 1.0\n",
        "        for r in recalls:\n",
        "            prod *= r\n",
        "        return float(prod ** (1.0 / len(recalls)))\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Dry-run sanity\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def dry_run_sanity(cfg: Config):\n",
        "    print(\"[Sanity] Building dataset and one batch...\")\n",
        "    ds = ROIPatchDataset(cfg)\n",
        "    # Choose steps so that DataLoader yields 1 batch quickly\n",
        "    sampler = FixedPatchBatchSampler(ds, cfg, steps_per_epoch=1)\n",
        "    def _collate(samples):\n",
        "        return collate_roi_batch(samples, cfg)\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_sampler=sampler,\n",
        "        collate_fn=_collate,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=cfg.pin_memory,\n",
        "        persistent_workers=cfg.persistent_workers,\n",
        "        prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
        "    )\n",
        "\n",
        "    batch = next(iter(loader))\n",
        "    P = batch[\"images\"].size(0)\n",
        "    R = batch[\"roi_targets\"].size(0)\n",
        "    print(f\"[Sanity] Batch patches={P}, ROIs={R}\")\n",
        "\n",
        "    # Model forward on CPU for shape checks (switch to CUDA in training script)\n",
        "    model = PatchRel(num_classes=3)\n",
        "    model.eval()\n",
        "    roi_logits, aux = model(batch[\"images\"], batch[\"roi_ids\"])  # [R,C]\n",
        "\n",
        "    # Check α sums ≈ 1 per (ROI,class)\n",
        "    alpha = aux[\"alpha\"]  # [P,C]\n",
        "    sums = torch.zeros((R, alpha.size(1)))\n",
        "    sums.index_add_(0, batch[\"roi_ids\"], alpha.cpu())\n",
        "    max_dev = (sums - 1.0).abs().max().item()\n",
        "    print(f\"[Sanity] max |∑_j α_jc − 1| across ROI×class = {max_dev:.3e}\")\n",
        "\n",
        "    # Loss shape check\n",
        "    crit = ROIBCELoss(num_classes=3)\n",
        "    loss = crit(roi_logits, batch[\"roi_targets\"])\n",
        "    print(f\"[Sanity] BCE loss (random init) = {float(loss):.4f}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Validation sampler (covers entire split deterministically)\n",
        "# -------------------------------\n",
        "class ValPatchBatchSampler(Sampler[List[int]]):\n",
        "    \"\"\"Sequentially iterate over all ROIs, greedily packing them into batches\n",
        "    under the same patch budget and per-ROI cap. Deterministic; no oversampling.\"\"\"\n",
        "    def __init__(self, dataset: ROIPatchDataset, cfg: Config):\n",
        "        self.ds = dataset\n",
        "        self.cfg = cfg\n",
        "        self.order = list(range(len(self.ds)))  # fixed order\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # Not strictly needed by PyTorch when using batch_sampler, but provide an estimate\n",
        "        # by simulating packs (cheap upper bound: number of ROIs)\n",
        "        return max(1, math.ceil(len(self.ds) / 4))\n",
        "\n",
        "    def __iter__(self):\n",
        "        i = 0\n",
        "        N = len(self.order)\n",
        "        while i < N:\n",
        "            patch_budget = self.cfg.batch_total_patches\n",
        "            this_batch: List[int] = []\n",
        "            while i < N:\n",
        "                roi_idx = self.order[i]\n",
        "                # compute capped length\n",
        "                L = min(len(self.ds.roi_items[roi_idx][2]), self.cfg.max_patches_per_roi)\n",
        "                if L <= 0:\n",
        "                    i += 1\n",
        "                    continue\n",
        "                if L > patch_budget and len(this_batch) > 0:\n",
        "                    break\n",
        "                this_batch.append(roi_idx)\n",
        "                patch_budget -= L\n",
        "                i += 1\n",
        "                if patch_budget <= 0:\n",
        "                    break\n",
        "            yield this_batch\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Metrics helpers\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def build_confusion(num_classes: int = 3) -> torch.Tensor:\n",
        "    return torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_confusion(conf: torch.Tensor, true_labels: torch.Tensor, pred_labels: torch.Tensor) -> None:\n",
        "    for t, p in zip(true_labels.view(-1), pred_labels.view(-1)):\n",
        "        conf[int(t), int(p)] += 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZ3nbzYrxGs1"
      },
      "id": "tZ3nbzYrxGs1",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "# -------------------------------\n",
        "class Trainer:\n",
        "    def __init__(self, cfg_train: Config, cfg_val: Config, steps_per_epoch: int = 500, out_dir: Path = Path(\"/content/drive/MyDrive/BRACS/checkpoints/PatchRel_noaug2\")):\n",
        "        self.cfg_train = cfg_train\n",
        "        self.cfg_val = cfg_val\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.out_dir = out_dir\n",
        "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Datasets\n",
        "        self.ds_train = ROIPatchDataset(cfg_train)\n",
        "        self.ds_val = ROIPatchDataset(cfg_val)\n",
        "\n",
        "        # Samplers\n",
        "        self.sampler_train = FixedPatchBatchSampler(self.ds_train, cfg_train, steps_per_epoch=self.steps_per_epoch)\n",
        "        self.sampler_val = ValPatchBatchSampler(self.ds_val, cfg_val)\n",
        "\n",
        "        # Loaders\n",
        "        self.loader_train = DataLoader(\n",
        "            self.ds_train,\n",
        "            batch_sampler=self.sampler_train,\n",
        "            collate_fn=lambda s: collate_roi_batch(s, cfg_train),\n",
        "            num_workers=cfg_train.num_workers,\n",
        "            pin_memory=cfg_train.pin_memory,\n",
        "            persistent_workers=cfg_train.persistent_workers,\n",
        "            prefetch_factor=2 if cfg_train.num_workers > 0 else None,\n",
        "        )\n",
        "        self.loader_val = DataLoader(\n",
        "            self.ds_val,\n",
        "            batch_sampler=self.sampler_val,\n",
        "            collate_fn=lambda s: collate_roi_batch(s, cfg_val),\n",
        "            num_workers=cfg_val.num_workers,\n",
        "            pin_memory=cfg_val.pin_memory,\n",
        "            persistent_workers=cfg_val.persistent_workers,\n",
        "            prefetch_factor=2 if cfg_val.num_workers > 0 else None,\n",
        "        )\n",
        "\n",
        "        # Model & optim\n",
        "        self.model = PatchRel(num_classes=3).to(self.device)\n",
        "        self.crit = ROIBCELoss(num_classes=3)\n",
        "        self.opt = torch.optim.SGD(self.model.parameters(), lr=3e-2, momentum=0.75, weight_decay=1e-4)\n",
        "        # use new AMP API\n",
        "        self.scaler = torch.amp.GradScaler('cuda', enabled=(self.device.type == 'cuda'))\n",
        "\n",
        "        self.best = {\"gmean\": -1.0, \"epoch\": -1}\n",
        "\n",
        "    def train_one_epoch(self, epoch: int) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_class_counts = torch.zeros(3, dtype=torch.long)\n",
        "\n",
        "        pbar = tqdm(total=self.steps_per_epoch, desc=f\"Epoch {epoch:02d} [train]\", leave=False)\n",
        "        for step, batch in enumerate(self.loader_train, start=1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            imgs = batch[\"images\"].to(self.device, non_blocking=True)\n",
        "            roi_ids = batch[\"roi_ids\"].to(self.device)\n",
        "            roi_targets = batch[\"roi_targets\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                epoch_class_counts.index_add_(0, roi_targets.cpu(), torch.ones_like(roi_targets.cpu()))\n",
        "\n",
        "            self.opt.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda', enabled=(self.device.type == 'cuda')):\n",
        "                roi_logits, _ = self.model(imgs, roi_ids)\n",
        "                loss = self.crit(roi_logits, roi_targets)\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.opt)\n",
        "            self.scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # progress bar update\n",
        "            patches = int(imgs.size(0))\n",
        "            dt = max(time.time() - t0, 1e-6)\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"patches\": patches, \"pps\": f\"{patches/dt:.1f}\"})\n",
        "            pbar.update(1)\n",
        "\n",
        "            if step >= self.steps_per_epoch:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        avg_loss = running_loss / max(1, self.steps_per_epoch)\n",
        "        counts = {SUPERCLASSES[i]: int(epoch_class_counts[i].item()) for i in range(3)}\n",
        "        return {\"loss\": avg_loss, \"counts_B\": counts[\"B\"], \"counts_A\": counts[\"A\"], \"counts_M\": counts[\"M\"]}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self) -> Tuple[float, torch.Tensor]:\n",
        "        self.model.eval()\n",
        "        conf = build_confusion(num_classes=3).to(self.device)\n",
        "        for batch in self.loader_val:\n",
        "            imgs = batch[\"images\"].to(self.device, non_blocking=True)\n",
        "            roi_ids = batch[\"roi_ids\"].to(self.device)\n",
        "            roi_targets = batch[\"roi_targets\"].to(self.device)\n",
        "            with torch.amp.autocast('cuda', enabled=(self.device.type == 'cuda')):\n",
        "                roi_logits, _ = self.model(imgs, roi_ids)\n",
        "                # For 3-way decision, use argmax over sigmoid probabilities\n",
        "                probs = roi_logits.sigmoid()\n",
        "                preds = probs.argmax(dim=1)\n",
        "            update_confusion(conf, roi_targets, preds)\n",
        "        g = gmean_from_confusion(conf.cpu())\n",
        "        return g, conf.cpu()\n",
        "\n",
        "    def save_ckpt(self, epoch: int, is_best: bool = False) -> None:\n",
        "        path = self.out_dir / (\"epoch_%03d.pt\" % epoch)\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model\": self.model.state_dict(),\n",
        "            \"optimizer\": self.opt.state_dict(),\n",
        "            \"scaler\": self.scaler.state_dict(),\n",
        "            \"meta\": {\n",
        "                \"normalize_mean\": self.cfg_train.normalize_mean,\n",
        "                \"normalize_std\": self.cfg_train.normalize_std,\n",
        "                \"image_size\": self.cfg_train.image_size,\n",
        "                \"superclasses\": SUPERCLASSES,\n",
        "            },\n",
        "        }, path)\n",
        "        if is_best:\n",
        "            best_path = self.out_dir / \"best_by_gmean.pt\"\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model\": self.model.state_dict(),\n",
        "                \"meta\": {\n",
        "                    \"normalize_mean\": self.cfg_train.normalize_mean,\n",
        "                    \"normalize_std\": self.cfg_train.normalize_std,\n",
        "                    \"image_size\": self.cfg_train.image_size,\n",
        "                    \"superclasses\": SUPERCLASSES,\n",
        "                    \"best_gmean\": self.best[\"gmean\"],\n",
        "                },\n",
        "            }, best_path)\n",
        "\n",
        "    def fit(self, num_epochs: int = 40, save_every: int = 2):\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            print(f\"Epoch {epoch:02d} — training...\")\n",
        "            tr = self.train_one_epoch(epoch)\n",
        "\n",
        "            print(\"  running validation...\")\n",
        "            val_g, conf = self.validate()\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch:02d} | train_loss={tr['loss']:.4f} | val_gmean={val_g:.4f} | \"\n",
        "                f\"batched_ROIs_per_class B/A/M=({tr['counts_B']},{tr['counts_A']},{tr['counts_M']})\"\n",
        "            )\n",
        "\n",
        "            if (epoch % save_every) == 0:\n",
        "                self.save_ckpt(epoch, is_best=False)\n",
        "\n",
        "            if val_g > self.best[\"gmean\"]:\n",
        "                self.best.update({\"gmean\": val_g, \"epoch\": epoch})\n",
        "                self.save_ckpt(epoch, is_best=True)\n",
        "                print(f\"  ↳ saved BEST checkpoint @ epoch {epoch} (g-mean={val_g:.4f})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Speed hint for first run\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    train_cfg = Config(\n",
        "        drive_root=Path(\"/content/drive/MyDrive/BRACS/ROIPatches\"),\n",
        "        split=\"train\",\n",
        "        batch_total_patches=512,\n",
        "        max_patches_per_roi=20,\n",
        "        num_workers=8,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        fast_io_cap=20,   # align dataset reads to cap to reduce Drive I/O\n",
        "    )\n",
        "    val_cfg = Config(\n",
        "        drive_root=Path(\"/content/drive/MyDrive/BRACS/ROIPatches\"),\n",
        "        split=\"val\",\n",
        "        batch_total_patches=512,\n",
        "        max_patches_per_roi=20,\n",
        "        num_workers=8,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        fast_io_cap=20,\n",
        "    )\n",
        "\n",
        "    # Optionally sanity check\n",
        "    # dry_run_sanity(train_cfg)\n",
        "\n",
        "    trainer = Trainer(train_cfg, val_cfg, steps_per_epoch=180)\n",
        "    trainer.fit(num_epochs=40, save_every=2)"
      ],
      "metadata": {
        "id": "K6CkfrouxS9o",
        "outputId": "32641be0-383e-4401-c076-448baa697350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ebe36428a4614451889718d2b2fc567e",
            "d254cbeecabb4a69934a7ed2a7595d39",
            "0f45b78251334f3a8d8b908fa58c248a",
            "8dfdc0739d8040ebb6d81cf60885ffc6",
            "938610a996a843e7958a25a753ce6838",
            "7b64f8900b3b436fa05316f0a37fb0af",
            "1a8073ac6ef24ed2b4e6da0104772a2a",
            "54ecfa92fc8144ed88ee5917ada78b54",
            "1978284b50da4c598f9b5c6f4596f6e7",
            "4fe64f4db4794fc7aef91dad161e2ba9",
            "bef669f23a3a4e05824370269f5641f8"
          ]
        }
      },
      "id": "K6CkfrouxS9o",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Index] Split=train ROIs=2084 per-class={'B': 911, 'A': 250, 'M': 923} (patches~93938)\n",
            "[Index] Split=val ROIs=178 per-class={'B': 64, 'A': 31, 'M': 83} (patches~9197)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebe36428a4614451889718d2b2fc567e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 01 [train]:   0%|          | 0/180 [00:00<?, ?it/s]/tmp/ipython-input-1106626708.py:329: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/Indexing.cu:1432.)\n",
            "  max_per_roi_class.index_reduce_(0, roi_ids, logits, reduce=\"amax\")  # [R, C]\n",
            "                                                                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=0.2194 | val_gmean=0.8747 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "  ↳ saved BEST checkpoint @ epoch 1 (g-mean=0.8747)\n",
            "Epoch 02 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 02 | train_loss=0.0853 | val_gmean=0.8754 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "  ↳ saved BEST checkpoint @ epoch 2 (g-mean=0.8754)\n",
            "Epoch 03 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 03 | train_loss=0.0559 | val_gmean=0.8375 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 04 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 04 | train_loss=0.0392 | val_gmean=0.8066 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 05 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 05 | train_loss=0.0315 | val_gmean=0.7889 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 06 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 06 | train_loss=0.0333 | val_gmean=0.8144 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 07 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 07 | train_loss=0.0189 | val_gmean=0.8054 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 08 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 08 | train_loss=0.0236 | val_gmean=0.6125 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 09 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 09 | train_loss=0.0281 | val_gmean=0.7882 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 10 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 10 | train_loss=0.0219 | val_gmean=0.6603 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 11 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 11 | train_loss=0.0093 | val_gmean=0.6486 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 12 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 12 | train_loss=0.0074 | val_gmean=0.7161 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 13 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 13 | train_loss=0.0133 | val_gmean=0.7610 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 14 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 14 | train_loss=0.0065 | val_gmean=0.6534 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 15 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 15 | train_loss=0.0046 | val_gmean=0.7199 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 16 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 16 | train_loss=0.0028 | val_gmean=0.7169 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 17 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 17 | train_loss=0.0013 | val_gmean=0.7022 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 18 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 18 | train_loss=0.0010 | val_gmean=0.6964 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 19 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 19 | train_loss=0.0013 | val_gmean=0.7071 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 20 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 20 | train_loss=0.0006 | val_gmean=0.7221 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 21 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 21 | train_loss=0.0006 | val_gmean=0.6964 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 22 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 22 | train_loss=0.0009 | val_gmean=0.6711 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 23 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 23 | train_loss=0.0004 | val_gmean=0.6803 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 24 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 24 | train_loss=0.0006 | val_gmean=0.6595 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 25 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 25 | train_loss=0.0007 | val_gmean=0.7386 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 26 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 26 | train_loss=0.0031 | val_gmean=0.7561 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 27 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 27 | train_loss=0.0040 | val_gmean=0.7981 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 28 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 28 | train_loss=0.0021 | val_gmean=0.7849 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 29 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 29 | train_loss=0.0030 | val_gmean=0.7153 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 30 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 30 | train_loss=0.0080 | val_gmean=0.7254 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 31 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 31 | train_loss=0.0156 | val_gmean=0.7285 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 32 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 32 | train_loss=0.0048 | val_gmean=0.7424 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 33 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 33 | train_loss=0.0030 | val_gmean=0.7714 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 34 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 34 | train_loss=0.0085 | val_gmean=0.7515 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 35 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 35 | train_loss=0.0041 | val_gmean=0.7682 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 36 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 36 | train_loss=0.0017 | val_gmean=0.7014 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 37 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 37 | train_loss=0.0019 | val_gmean=0.7816 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 38 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 38 | train_loss=0.0020 | val_gmean=0.7579 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 39 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 39 | train_loss=0.0015 | val_gmean=0.7199 | batched_ROIs_per_class B/A/M=(720,2160,720)\n",
            "Epoch 40 — training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  running validation...\n",
            "Epoch 40 | train_loss=0.0016 | val_gmean=0.7436 | batched_ROIs_per_class B/A/M=(720,2160,720)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebe36428a4614451889718d2b2fc567e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d254cbeecabb4a69934a7ed2a7595d39",
              "IPY_MODEL_0f45b78251334f3a8d8b908fa58c248a",
              "IPY_MODEL_8dfdc0739d8040ebb6d81cf60885ffc6"
            ],
            "layout": "IPY_MODEL_938610a996a843e7958a25a753ce6838"
          }
        },
        "d254cbeecabb4a69934a7ed2a7595d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b64f8900b3b436fa05316f0a37fb0af",
            "placeholder": "​",
            "style": "IPY_MODEL_1a8073ac6ef24ed2b4e6da0104772a2a",
            "value": "model.safetensors: 100%"
          }
        },
        "0f45b78251334f3a8d8b908fa58c248a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ecfa92fc8144ed88ee5917ada78b54",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1978284b50da4c598f9b5c6f4596f6e7",
            "value": 21355344
          }
        },
        "8dfdc0739d8040ebb6d81cf60885ffc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe64f4db4794fc7aef91dad161e2ba9",
            "placeholder": "​",
            "style": "IPY_MODEL_bef669f23a3a4e05824370269f5641f8",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 40.9MB/s]"
          }
        },
        "938610a996a843e7958a25a753ce6838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b64f8900b3b436fa05316f0a37fb0af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8073ac6ef24ed2b4e6da0104772a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54ecfa92fc8144ed88ee5917ada78b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1978284b50da4c598f9b5c6f4596f6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fe64f4db4794fc7aef91dad161e2ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef669f23a3a4e05824370269f5641f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}