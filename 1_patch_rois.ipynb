{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f58b84f3",
      "metadata": {
        "id": "f58b84f3"
      },
      "source": [
        "# roi patching pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Deps (Colab) ---\n",
        "!pip -q install google-cloud-storage pillow tqdm\n",
        "\n",
        "import io, time, gc, os, concurrent.futures\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import auth as colab_auth, drive as colab_drive\n",
        "import google.auth\n",
        "from google.cloud import storage\n",
        "\n",
        "# -------------------\n",
        "# Auth & Mounts\n",
        "# -------------------\n",
        "# GCS auth for source reads\n",
        "colab_auth.authenticate_user()\n",
        "creds, _ = google.auth.default(scopes=['https://www.googleapis.com/auth/devstorage.read_write'])\n",
        "client = storage.Client(credentials=creds)\n",
        "\n",
        "# Mount Google Drive for destination writes\n",
        "colab_drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# -------------------\n",
        "# Config\n",
        "# -------------------\n",
        "BUCKET_NAME = \"bracs-dataset-bucket\"\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "# SOURCE: GCS (unchanged)\n",
        "SRC_PREFIX = \"BRACS/BRACS_RoI/latest_version\"   # ROIs source\n",
        "\n",
        "# DESTINATION: GOOGLE DRIVE\n",
        "DEST_DRIVE_ROOT = \"/content/drive/MyDrive/BRACS/ROIPatches\"  # <<< NEW\n",
        "\n",
        "SPLITS  = [\"train\",\"val\",\"test\"]\n",
        "LESIONS = [\"0_N\",\"1_PB\",\"2_UDH\",\"3_FEA\",\"4_ADH\",\"5_DCIS\",\"6_IC\"]\n",
        "\n",
        "PATCH_SIZE = 224\n",
        "STRIDE     = 74\n",
        "DOWNSCALE_FACTOR = 4   # 40x -> 10x\n",
        "\n",
        "# GPU / perf\n",
        "GPU_DEVICE = \"cuda:0\"\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Threading\n",
        "ENCODE_WRITE_WORKERS = 8\n",
        "RETRY_DOWNLOADS = 5\n",
        "RETRY_WRITES    = 5\n",
        "\n",
        "# Batch ROIs (looping granularity)\n",
        "BATCH_ROIS = 32\n",
        "\n",
        "# Prevent DecompressionBomb warnings\n",
        "Image.MAX_IMAGE_PIXELS = 300_000_000\n",
        "\n",
        "# -------------------\n",
        "# Helpers\n",
        "# -------------------\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def parse_ids(blob_name: str):\n",
        "    p = Path(blob_name)\n",
        "    # .../latest_version/<split>/<lesion>/<roi>.png\n",
        "    return p.parent.parent.name, p.parent.name, p.stem  # split, lesion, roi_id\n",
        "\n",
        "def list_roi_blobs() -> List[str]:\n",
        "    names = []\n",
        "    for split in SPLITS:\n",
        "        for lesion in LESIONS:\n",
        "            prefix = f\"{SRC_PREFIX}/{split}/{lesion}/\"\n",
        "            for b in client.list_blobs(BUCKET_NAME, prefix=prefix):\n",
        "                if b.name.lower().endswith(\".png\") and Path(b.name).parent.name == lesion:\n",
        "                    names.append(b.name)\n",
        "    return names\n",
        "\n",
        "def roi_already_done_drive(split: str, lesion: str, roi_id: str) -> bool:\n",
        "    \"\"\"Treat ROI as done if destination folder exists and has at least 1 file.\"\"\"\n",
        "    dest_dir = Path(DEST_DRIVE_ROOT) / split / lesion / roi_id\n",
        "    if not dest_dir.exists():\n",
        "        return False\n",
        "    try:\n",
        "        # any() short-circuits; avoids listing huge dirs\n",
        "        return any(dest_dir.iterdir())\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def download_bytes_with_retry(blob_name: str, retries=RETRY_DOWNLOADS):\n",
        "    b = bucket.blob(blob_name)\n",
        "    delay = 1.0\n",
        "    for k in range(retries):\n",
        "        try:\n",
        "            return b.download_as_bytes()\n",
        "        except Exception as e:\n",
        "            if k == retries - 1:\n",
        "                raise\n",
        "            time.sleep(delay); delay *= 1.7\n",
        "\n",
        "def write_bytes_with_retry(path: Path, data: bytes, retries=RETRY_WRITES):\n",
        "    delay = 1.0\n",
        "    for k in range(retries):\n",
        "        try:\n",
        "            ensure_dir(path.parent)\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "            return\n",
        "        except Exception as e:\n",
        "            if k == retries - 1:\n",
        "                raise\n",
        "            time.sleep(delay); delay *= 1.7\n",
        "\n",
        "def decode_image_bytes(img_bytes: bytes) -> np.ndarray:\n",
        "    return np.array(Image.open(io.BytesIO(img_bytes)).convert(\"RGB\"))\n",
        "\n",
        "def downscale_to_10x(img_np: np.ndarray, factor: int = DOWNSCALE_FACTOR) -> np.ndarray:\n",
        "    h, w = img_np.shape[:2]\n",
        "    nh, nw = max(1, h // factor), max(1, w // factor)\n",
        "    return np.array(Image.fromarray(img_np).resize((nw, nh), resample=Image.BOX))\n",
        "\n",
        "def encode_png(np_img: np.ndarray) -> bytes:\n",
        "    bio = io.BytesIO()\n",
        "    Image.fromarray(np_img).save(bio, format=\"PNG\", optimize=True)\n",
        "    return bio.getvalue()\n",
        "\n",
        "# -------------------\n",
        "# Core per-ROI processing (GPU)\n",
        "# -------------------\n",
        "@torch.no_grad()\n",
        "def process_one_roi(blob_name: str):\n",
        "    \"\"\"Returns (num_patches_saved, skipped) for this ROI; saves to Drive.\"\"\"\n",
        "    split, lesion, roi_id = parse_ids(blob_name)\n",
        "\n",
        "    # Skip if already done on Drive\n",
        "    if roi_already_done_drive(split, lesion, roi_id):\n",
        "        return 0, True  # 0 saved, skipped\n",
        "\n",
        "    # Download ROI from GCS\n",
        "    try:\n",
        "        roi_bytes = download_bytes_with_retry(blob_name)\n",
        "    except Exception:\n",
        "        return 0, False\n",
        "\n",
        "    # Decode @40x -> downscale to 10x\n",
        "    try:\n",
        "        img40 = decode_image_bytes(roi_bytes)\n",
        "        img   = downscale_to_10x(img40, DOWNSCALE_FACTOR)\n",
        "        del img40, roi_bytes\n",
        "    except Exception:\n",
        "        return 0, False\n",
        "\n",
        "    ks, st = PATCH_SIZE, STRIDE\n",
        "    H, W, _ = img.shape\n",
        "    if H < ks or W < ks:\n",
        "        del img\n",
        "        return 0, True  # treat as processed (nothing to do)\n",
        "\n",
        "    # To GPU, unfold patches\n",
        "    device = torch.device(GPU_DEVICE)\n",
        "    x = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).to(device, dtype=torch.float32, non_blocking=True) / 255.0\n",
        "    del img\n",
        "\n",
        "    unf = F.unfold(x, kernel_size=ks, stride=st)  # 1 x (3*ks*ks) x L\n",
        "    L = unf.shape[-1]\n",
        "    if L == 0:\n",
        "        del x, unf\n",
        "        torch.cuda.empty_cache()\n",
        "        return 0, True\n",
        "\n",
        "    patches = unf.squeeze(0).transpose(0,1).reshape(L, 3, ks, ks).contiguous()\n",
        "\n",
        "    # quick tissue filter (GPU)\n",
        "    brightness = patches.mean(dim=(1,2,3))\n",
        "    sat        = patches.std(dim=(1,2,3))\n",
        "    keep_mask  = (brightness < 0.85) & (sat > 0.02)\n",
        "    keep_idx   = torch.nonzero(keep_mask).squeeze(1)\n",
        "    if keep_idx.numel() == 0:\n",
        "        del x, unf, patches, brightness, sat, keep_mask, keep_idx\n",
        "        torch.cuda.empty_cache()\n",
        "        return 0, True\n",
        "\n",
        "    kept = patches[keep_idx]\n",
        "    cols = (W - ks)//st + 1\n",
        "    ys_all = (torch.arange(L, device=device) // cols) * st\n",
        "    xs_all = (torch.arange(L, device=device) %  cols) * st\n",
        "    ys = ys_all[keep_idx].cpu().numpy().tolist()\n",
        "    xs = xs_all[keep_idx].cpu().numpy().tolist()\n",
        "\n",
        "    # Prepare Drive destination\n",
        "    dest_dir = Path(DEST_DRIVE_ROOT) / split / lesion / roi_id\n",
        "    ensure_dir(dest_dir)\n",
        "\n",
        "    # Encode + write on CPU threads\n",
        "    K = kept.shape[0]\n",
        "\n",
        "    def encode_and_write(i):\n",
        "        # GPU->CPU copy (non_blocking) then encode/write\n",
        "        arr = (kept[i].permute(1,2,0).cpu().numpy() * 255.0).astype(\"uint8\")\n",
        "        payload = encode_png(arr)\n",
        "        out_path = dest_dir / f\"patch_y{ys[i]}_x{xs[i]}.png\"\n",
        "        write_bytes_with_retry(out_path, payload, retries=RETRY_WRITES)\n",
        "\n",
        "    saved = 0\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=ENCODE_WRITE_WORKERS) as ex:\n",
        "        futures = [ex.submit(encode_and_write, i) for i in range(K)]\n",
        "        for fut in concurrent.futures.as_completed(futures):\n",
        "            try:\n",
        "                fut.result()\n",
        "                saved += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    del x, unf, patches, brightness, sat, keep_mask, keep_idx, kept\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return saved, False  # saved count, not skipped\n",
        "\n",
        "# -------------------\n",
        "# Run (Batched)\n",
        "# -------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(GPU_DEVICE)\n",
        "    assert torch.cuda.is_available(), \"No CUDA device visible.\"\n",
        "    torch.cuda.set_device(device)\n",
        "\n",
        "    all_blobs = list_roi_blobs()\n",
        "    total = len(all_blobs)\n",
        "    done = 0\n",
        "    saved_total = 0\n",
        "    skipped_total = 0\n",
        "\n",
        "    for start in range(0, total, BATCH_ROIS):\n",
        "        batch = all_blobs[start : start + BATCH_ROIS]\n",
        "        for blob_name in batch:\n",
        "            saved, skipped = process_one_roi(blob_name)\n",
        "            done += 1\n",
        "            saved_total += saved\n",
        "            skipped_total += 1 if skipped else 0\n",
        "        print(f\"PROGRESS: {done}/{total} saved_patches={saved_total} skipped_rois={skipped_total}\")\n",
        "\n",
        "    print(\"FINISHED:\",\n",
        "          f\"ROIs={total}, saved_patches={saved_total}, skipped_rois={skipped_total}\")\n"
      ],
      "metadata": {
        "id": "3bN4e59ixKUe",
        "outputId": "8245e0e9-355d-41dd-fb14-292750d08c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3bN4e59ixKUe",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "PROGRESS: 32/4539 saved_patches=310 skipped_rois=5\n",
            "PROGRESS: 64/4539 saved_patches=1400 skipped_rois=5\n",
            "PROGRESS: 96/4539 saved_patches=2268 skipped_rois=6\n",
            "PROGRESS: 128/4539 saved_patches=3210 skipped_rois=6\n",
            "PROGRESS: 160/4539 saved_patches=3829 skipped_rois=7\n",
            "PROGRESS: 192/4539 saved_patches=4547 skipped_rois=9\n",
            "PROGRESS: 224/4539 saved_patches=5243 skipped_rois=14\n",
            "PROGRESS: 256/4539 saved_patches=5435 skipped_rois=33\n",
            "PROGRESS: 288/4539 saved_patches=6014 skipped_rois=44\n",
            "PROGRESS: 320/4539 saved_patches=6336 skipped_rois=50\n",
            "PROGRESS: 352/4539 saved_patches=6846 skipped_rois=52\n",
            "PROGRESS: 384/4539 saved_patches=15065 skipped_rois=56\n",
            "PROGRESS: 416/4539 saved_patches=15508 skipped_rois=63\n",
            "PROGRESS: 448/4539 saved_patches=16736 skipped_rois=63\n",
            "PROGRESS: 480/4539 saved_patches=20047 skipped_rois=63\n",
            "PROGRESS: 512/4539 saved_patches=22003 skipped_rois=63\n",
            "PROGRESS: 544/4539 saved_patches=23372 skipped_rois=63\n",
            "PROGRESS: 576/4539 saved_patches=25843 skipped_rois=63\n",
            "PROGRESS: 608/4539 saved_patches=27233 skipped_rois=63\n",
            "PROGRESS: 640/4539 saved_patches=27861 skipped_rois=66\n",
            "PROGRESS: 672/4539 saved_patches=28780 skipped_rois=71\n",
            "PROGRESS: 704/4539 saved_patches=29513 skipped_rois=71\n",
            "PROGRESS: 736/4539 saved_patches=30626 skipped_rois=72\n",
            "PROGRESS: 768/4539 saved_patches=31427 skipped_rois=80\n",
            "PROGRESS: 800/4539 saved_patches=31896 skipped_rois=86\n",
            "PROGRESS: 832/4539 saved_patches=32261 skipped_rois=98\n",
            "PROGRESS: 864/4539 saved_patches=32895 skipped_rois=114\n",
            "PROGRESS: 896/4539 saved_patches=33650 skipped_rois=115\n",
            "PROGRESS: 928/4539 saved_patches=34841 skipped_rois=117\n",
            "PROGRESS: 960/4539 saved_patches=35573 skipped_rois=119\n",
            "PROGRESS: 992/4539 saved_patches=35853 skipped_rois=125\n",
            "PROGRESS: 1024/4539 saved_patches=35958 skipped_rois=147\n",
            "PROGRESS: 1056/4539 saved_patches=37008 skipped_rois=149\n",
            "PROGRESS: 1088/4539 saved_patches=37259 skipped_rois=159\n",
            "PROGRESS: 1120/4539 saved_patches=37493 skipped_rois=171\n",
            "PROGRESS: 1152/4539 saved_patches=37921 skipped_rois=179\n",
            "PROGRESS: 1184/4539 saved_patches=38842 skipped_rois=188\n",
            "PROGRESS: 1216/4539 saved_patches=39050 skipped_rois=197\n",
            "PROGRESS: 1248/4539 saved_patches=39582 skipped_rois=202\n",
            "PROGRESS: 1280/4539 saved_patches=39755 skipped_rois=215\n",
            "PROGRESS: 1312/4539 saved_patches=39922 skipped_rois=236\n",
            "PROGRESS: 1344/4539 saved_patches=40243 skipped_rois=243\n",
            "PROGRESS: 1376/4539 saved_patches=40373 skipped_rois=258\n",
            "PROGRESS: 1408/4539 saved_patches=40801 skipped_rois=259\n",
            "PROGRESS: 1440/4539 saved_patches=40978 skipped_rois=272\n",
            "PROGRESS: 1472/4539 saved_patches=41446 skipped_rois=282\n",
            "PROGRESS: 1504/4539 saved_patches=41663 skipped_rois=310\n",
            "PROGRESS: 1536/4539 saved_patches=41708 skipped_rois=329\n",
            "PROGRESS: 1568/4539 saved_patches=41736 skipped_rois=345\n",
            "PROGRESS: 1600/4539 saved_patches=41775 skipped_rois=365\n",
            "PROGRESS: 1632/4539 saved_patches=41819 skipped_rois=384\n",
            "PROGRESS: 1664/4539 saved_patches=41909 skipped_rois=405\n",
            "PROGRESS: 1696/4539 saved_patches=41953 skipped_rois=425\n",
            "PROGRESS: 1728/4539 saved_patches=41984 skipped_rois=443\n",
            "PROGRESS: 1760/4539 saved_patches=42022 skipped_rois=463\n",
            "PROGRESS: 1792/4539 saved_patches=42073 skipped_rois=479\n",
            "PROGRESS: 1824/4539 saved_patches=42284 skipped_rois=490\n",
            "PROGRESS: 1856/4539 saved_patches=42365 skipped_rois=502\n",
            "PROGRESS: 1888/4539 saved_patches=42602 skipped_rois=506\n",
            "PROGRESS: 1920/4539 saved_patches=42707 skipped_rois=519\n",
            "PROGRESS: 1952/4539 saved_patches=42850 skipped_rois=530\n",
            "PROGRESS: 1984/4539 saved_patches=43135 skipped_rois=537\n",
            "PROGRESS: 2016/4539 saved_patches=43157 skipped_rois=563\n",
            "PROGRESS: 2048/4539 saved_patches=43172 skipped_rois=589\n",
            "PROGRESS: 2080/4539 saved_patches=43253 skipped_rois=604\n",
            "PROGRESS: 2112/4539 saved_patches=43315 skipped_rois=626\n",
            "PROGRESS: 2144/4539 saved_patches=43497 skipped_rois=640\n",
            "PROGRESS: 2176/4539 saved_patches=43803 skipped_rois=647\n",
            "PROGRESS: 2208/4539 saved_patches=44000 skipped_rois=657\n",
            "PROGRESS: 2240/4539 saved_patches=44716 skipped_rois=657\n",
            "PROGRESS: 2272/4539 saved_patches=44865 skipped_rois=667\n",
            "PROGRESS: 2304/4539 saved_patches=45383 skipped_rois=673\n",
            "PROGRESS: 2336/4539 saved_patches=45807 skipped_rois=675\n",
            "PROGRESS: 2368/4539 saved_patches=46026 skipped_rois=677\n",
            "PROGRESS: 2400/4539 saved_patches=46195 skipped_rois=681\n",
            "PROGRESS: 2432/4539 saved_patches=46275 skipped_rois=698\n",
            "PROGRESS: 2464/4539 saved_patches=46367 skipped_rois=711\n",
            "PROGRESS: 2496/4539 saved_patches=47826 skipped_rois=713\n",
            "PROGRESS: 2528/4539 saved_patches=50170 skipped_rois=713\n",
            "PROGRESS: 2560/4539 saved_patches=50532 skipped_rois=724\n",
            "PROGRESS: 2592/4539 saved_patches=52653 skipped_rois=726\n",
            "PROGRESS: 2624/4539 saved_patches=53528 skipped_rois=728\n",
            "PROGRESS: 2656/4539 saved_patches=54206 skipped_rois=730\n",
            "PROGRESS: 2688/4539 saved_patches=55653 skipped_rois=730\n",
            "PROGRESS: 2720/4539 saved_patches=57081 skipped_rois=730\n",
            "PROGRESS: 2752/4539 saved_patches=58175 skipped_rois=733\n",
            "PROGRESS: 2784/4539 saved_patches=58751 skipped_rois=733\n",
            "PROGRESS: 2816/4539 saved_patches=59245 skipped_rois=740\n",
            "PROGRESS: 2848/4539 saved_patches=59281 skipped_rois=761\n",
            "PROGRESS: 2880/4539 saved_patches=59308 skipped_rois=788\n",
            "PROGRESS: 2912/4539 saved_patches=60179 skipped_rois=789\n",
            "PROGRESS: 2944/4539 saved_patches=61133 skipped_rois=789\n",
            "PROGRESS: 2976/4539 saved_patches=62290 skipped_rois=790\n",
            "PROGRESS: 3008/4539 saved_patches=63085 skipped_rois=793\n",
            "PROGRESS: 3040/4539 saved_patches=63957 skipped_rois=794\n",
            "PROGRESS: 3072/4539 saved_patches=64850 skipped_rois=796\n",
            "PROGRESS: 3104/4539 saved_patches=65362 skipped_rois=799\n",
            "PROGRESS: 3136/4539 saved_patches=65782 skipped_rois=805\n",
            "PROGRESS: 3168/4539 saved_patches=69617 skipped_rois=809\n",
            "PROGRESS: 3200/4539 saved_patches=72973 skipped_rois=809\n",
            "PROGRESS: 3232/4539 saved_patches=75919 skipped_rois=809\n",
            "PROGRESS: 3264/4539 saved_patches=77669 skipped_rois=809\n",
            "PROGRESS: 3296/4539 saved_patches=78107 skipped_rois=820\n",
            "PROGRESS: 3328/4539 saved_patches=78367 skipped_rois=825\n",
            "PROGRESS: 3360/4539 saved_patches=78600 skipped_rois=828\n",
            "PROGRESS: 3392/4539 saved_patches=80571 skipped_rois=830\n",
            "PROGRESS: 3424/4539 saved_patches=82557 skipped_rois=830\n",
            "PROGRESS: 3456/4539 saved_patches=83470 skipped_rois=830\n",
            "PROGRESS: 3488/4539 saved_patches=84390 skipped_rois=837\n",
            "PROGRESS: 3520/4539 saved_patches=85844 skipped_rois=838\n",
            "PROGRESS: 3552/4539 saved_patches=87736 skipped_rois=838\n",
            "PROGRESS: 3584/4539 saved_patches=89523 skipped_rois=838\n",
            "PROGRESS: 3616/4539 saved_patches=92034 skipped_rois=838\n",
            "PROGRESS: 3648/4539 saved_patches=94898 skipped_rois=838\n",
            "PROGRESS: 3680/4539 saved_patches=96314 skipped_rois=838\n",
            "PROGRESS: 3712/4539 saved_patches=96717 skipped_rois=849\n",
            "PROGRESS: 3744/4539 saved_patches=97222 skipped_rois=861\n",
            "PROGRESS: 3776/4539 saved_patches=97357 skipped_rois=874\n",
            "PROGRESS: 3808/4539 saved_patches=98046 skipped_rois=883\n",
            "PROGRESS: 3840/4539 saved_patches=98198 skipped_rois=894\n",
            "PROGRESS: 3872/4539 saved_patches=98408 skipped_rois=909\n",
            "PROGRESS: 3904/4539 saved_patches=99289 skipped_rois=911\n",
            "PROGRESS: 3936/4539 saved_patches=101915 skipped_rois=912\n",
            "PROGRESS: 3968/4539 saved_patches=104958 skipped_rois=912\n",
            "PROGRESS: 4000/4539 saved_patches=105375 skipped_rois=917\n",
            "PROGRESS: 4032/4539 saved_patches=105649 skipped_rois=932\n",
            "PROGRESS: 4064/4539 saved_patches=105979 skipped_rois=943\n",
            "PROGRESS: 4096/4539 saved_patches=106620 skipped_rois=949\n",
            "PROGRESS: 4128/4539 saved_patches=106992 skipped_rois=960\n",
            "PROGRESS: 4160/4539 saved_patches=107234 skipped_rois=974\n",
            "PROGRESS: 4192/4539 saved_patches=107595 skipped_rois=987\n",
            "PROGRESS: 4224/4539 saved_patches=107962 skipped_rois=995\n",
            "PROGRESS: 4256/4539 saved_patches=108018 skipped_rois=1016\n",
            "PROGRESS: 4288/4539 saved_patches=108038 skipped_rois=1041\n",
            "PROGRESS: 4320/4539 saved_patches=108308 skipped_rois=1058\n",
            "PROGRESS: 4352/4539 saved_patches=108880 skipped_rois=1064\n",
            "PROGRESS: 4384/4539 saved_patches=108980 skipped_rois=1088\n",
            "PROGRESS: 4416/4539 saved_patches=109856 skipped_rois=1090\n",
            "PROGRESS: 4448/4539 saved_patches=110227 skipped_rois=1108\n",
            "PROGRESS: 4480/4539 saved_patches=113704 skipped_rois=1118\n",
            "PROGRESS: 4512/4539 saved_patches=114762 skipped_rois=1123\n",
            "PROGRESS: 4539/4539 saved_patches=115744 skipped_rois=1132\n",
            "FINISHED: ROIs=4539, saved_patches=115744, skipped_rois=1132\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}