{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c464961",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "3c464961"
      },
      "outputs": [],
      "source": [
        "# patch encoder training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cuda as cuda_backends\n",
        "\n",
        "# GPU math speedups (safe on A100/Ampere+)\n",
        "torch.set_float32_matmul_precision(\"high\")  # enables TF32 matmuls under the hood\n",
        "cuda_backends.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "TCvPAEMGwel0"
      },
      "id": "TCvPAEMGwel0",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SvfIYbyYRth",
        "outputId": "44e0cdba-1bf9-49e1-9274-8cde93999217"
      },
      "id": "0SvfIYbyYRth",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/BRACS/ROIPatches\"\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE_PATCHES = 512\n",
        "MAX_PATCHES_PER_ROI = 30\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "# training schedule\n",
        "EPOCHS = 40\n",
        "STEPS_PER_EPOCH = 180\n",
        "AMP = True\n",
        "GRAD_CLIP = 5.0\n",
        "\n",
        "\n",
        "# optimizer and regularization\n",
        "BASE_LR = 3e-2\n",
        "MOMENTUM = 0.75\n",
        "WEIGHT_DECAY = 1e-4\n",
        "DROPOUT_P = 0.1\n",
        "\n",
        "# !!! constant learning rate !!!\n",
        "# discriminative LRs & warm-up\n",
        "LR_DIV_FACTOR = 2.0          # deeper group -> larger LR by factor^k (we invert by distance to end)\n",
        "WARMUP_EPOCHS = 2\n",
        "FINETUNE_LR_DIVISOR = 10.0    # post-warmup: LR := LR / FINETUNE_LR_DIVISOR\n",
        "\n",
        "# subtype mapping\n",
        "SEVEN_TO_THREE = {\n",
        "    \"0_N\":   0, \"1_PB\": 0, \"2_UDH\": 0,  # Benign\n",
        "    \"3_FEA\": 1, \"4_ADH\": 1,            # Atypical\n",
        "    \"5_DCIS\":2, \"6_IC\":  2,            # Malignant\n",
        "}\n",
        "THREE_CLASS_NAMES = {0: \"Benign\", 1: \"Atypical\", 2: \"Malignant\"}\n",
        "\n",
        "# oversampling: boost atypical\n",
        "CLASS_BOOST = {0: 1.0, 1: 1.0, 2: 1.0}\n"
      ],
      "metadata": {
        "id": "Fhfow5U0ipDv"
      },
      "id": "Fhfow5U0ipDv",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Data indexing\n",
        "# -----------------------------\n",
        "def _is_img(p: Path):\n",
        "    return p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"}\n",
        "\n",
        "def scan_split_3way(split_dir: Path):\n",
        "    recs = []\n",
        "    if not split_dir.exists():\n",
        "        return recs\n",
        "    for seven_cls in sorted([d.name for d in split_dir.iterdir() if d.is_dir()]):\n",
        "        if seven_cls not in SEVEN_TO_THREE:\n",
        "            continue\n",
        "        y3 = SEVEN_TO_THREE[seven_cls]\n",
        "        cls_dir = split_dir / seven_cls\n",
        "        for roi_dir in cls_dir.iterdir():\n",
        "            if not roi_dir.is_dir():\n",
        "                continue\n",
        "            roi_id = f\"{seven_cls}/{roi_dir.name}\"\n",
        "            for img_path in roi_dir.iterdir():\n",
        "                if _is_img(img_path):\n",
        "                    recs.append({\"path\": str(img_path), \"y3\": y3, \"roi_id\": roi_id})\n",
        "    return recs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset & transforms\n",
        "# -----------------------------\n",
        "class PatchDataset(Dataset):\n",
        "    def __init__(self, records, transform):\n",
        "        self.records = records\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        img = Image.open(r[\"path\"]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, r[\"y3\"], r[\"roi_id\"]\n",
        "\n",
        "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "IMAGENET_MEAN = weights.transforms().mean\n",
        "IMAGENET_STD  = weights.transforms().std\n",
        "\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomApply([transforms.RandomRotation(degrees=(90, 270))], p=0.5),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.ColorJitter(hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])"
      ],
      "metadata": {
        "id": "edFm79gKmkI0"
      },
      "id": "edFm79gKmkI0",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Sampler (class-balanced + atypical boost)\n",
        "# -----------------------------\n",
        "def make_weighted_sampler_3way(records):\n",
        "    class_counts = Counter([r[\"y3\"] for r in records])\n",
        "    weights = []\n",
        "    for r in records:\n",
        "        y = r[\"y3\"]\n",
        "        w = CLASS_BOOST[y] * (1.0 / max(1, class_counts[y]))\n",
        "        weights.append(w)\n",
        "    return WeightedRandomSampler(weights, num_samples=len(records), replacement=True)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Collate: cap â‰¤30 patches/ROI\n",
        "# -----------------------------\n",
        "def _cap_per_roi(images, targets, rois, max_per_roi=30):\n",
        "    idxs_by_roi = defaultdict(list)\n",
        "    for i, roi in enumerate(rois):\n",
        "        idxs_by_roi[roi].append(i)\n",
        "    keep = []\n",
        "    for _, idxs in idxs_by_roi.items():\n",
        "        if len(idxs) <= max_per_roi:\n",
        "            keep.extend(idxs)\n",
        "        else:\n",
        "            keep.extend(random.sample(idxs, max_per_roi))\n",
        "    keep.sort()\n",
        "    images = images[keep]\n",
        "    targets = targets[keep]\n",
        "    rois = [rois[i] for i in keep]\n",
        "    return images, targets, rois\n",
        "\n",
        "def collate_fn_max30(batch):\n",
        "    imgs, ys, rois = [], [], []\n",
        "    for img, y, roi in batch:\n",
        "        imgs.append(img.unsqueeze(0))\n",
        "        ys.append(y)\n",
        "        rois.append(roi)\n",
        "    images = torch.cat(imgs, dim=0)\n",
        "    targets = torch.tensor(ys, dtype=torch.long)\n",
        "    images, targets, rois = _cap_per_roi(images, targets, rois, MAX_PATCHES_PER_ROI)\n",
        "    return images, targets, rois"
      ],
      "metadata": {
        "id": "fm3aM2NWpSzk"
      },
      "id": "fm3aM2NWpSzk",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Model (EfficientNet-B0)\n",
        "# -----------------------------\n",
        "class EfficientNetB0_3Way(nn.Module):\n",
        "    def __init__(self, num_classes=3, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        in_feat = self.backbone.classifier[1].in_features  # 1280\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "        self.pen_dropout = nn.Dropout(p=dropout_p)\n",
        "        self.classifier = nn.Linear(in_feat, num_classes)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_penultimate(self, x):\n",
        "        x = self.backbone.features(x)\n",
        "        x = self.backbone.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        pen = self.extract_penultimate(x)\n",
        "        pen = self.pen_dropout(pen)\n",
        "        logits = self.classifier(pen)\n",
        "        return logits, pen\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Optimizer param groups:\n",
        "#   - split EfficientNet features into depth-ordered groups\n",
        "#   - discriminative LRs\n",
        "#   - weight decay only on weights (no BN/bias)\n",
        "# -----------------------------\n",
        "def _params_weight_bias(mod):\n",
        "    weights, biases = [], []\n",
        "    for name, p in mod.named_parameters(recurse=False):\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if p.ndimension() == 1 or name.endswith(\".bias\"):\n",
        "            biases.append(p)\n",
        "        else:\n",
        "            weights.append(p)\n",
        "    # include children recursively\n",
        "    for child in mod.children():\n",
        "        w, b = _params_weight_bias(child)\n",
        "        weights.extend(w); biases.extend(b)\n",
        "    return weights, biases\n",
        "\n",
        "def build_discriminative_param_groups(model, base_lr, wd, mom, lr_div_factor):\n",
        "    \"\"\"\n",
        "    Split torchvision EfficientNet:\n",
        "      groups = [features[0], features[1], ..., features[N-1], classifier_head]\n",
        "    Earlier idx -> smaller LR; later idx -> larger; classifier largest.\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "\n",
        "    # break features sequential into groups\n",
        "    feats = model.backbone.features  # nn.Sequential([...])\n",
        "    for i in range(len(feats)):\n",
        "        weights, biases = _params_weight_bias(feats[i])\n",
        "        if weights or biases:\n",
        "            groups.append({\"weights\": weights, \"biases\": biases})\n",
        "\n",
        "    # add the classification head\n",
        "    head_w, head_b = _params_weight_bias(model.classifier)\n",
        "    groups.append({\"weights\": head_w, \"biases\": head_b})\n",
        "\n",
        "    # assign LRs (discriminative): earlier groups get lower LR\n",
        "    num_groups = len(groups)\n",
        "    optim_groups = []\n",
        "    for g_idx, g in enumerate(groups):\n",
        "        # distance from end: deeper groups -> larger LR\n",
        "        depth = num_groups - (g_idx + 1)  # 0 for last, increases as we go earlier\n",
        "        # divide by (lr_div_factor ** depth) to shrink earlier layers\n",
        "        cur_lr = base_lr / (lr_div_factor ** depth)\n",
        "        if g[\"weights\"]:\n",
        "            optim_groups.append({\"params\": g[\"weights\"], \"lr\": cur_lr, \"momentum\": mom, \"weight_decay\": wd})\n",
        "        if g[\"biases\"]:\n",
        "            optim_groups.append({\"params\": g[\"biases\"], \"lr\": cur_lr, \"momentum\": mom, \"weight_decay\": 0.0})\n",
        "    init_lrs = [pg[\"lr\"] for pg in optim_groups]\n",
        "    return optim_groups, init_lrs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics (per-class recall & g-mean)\n",
        "# -----------------------------\n",
        "def per_class_recall(preds, targets, num_classes=3):\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    recalls = []\n",
        "    for c in range(num_classes):\n",
        "        idx = (targets == c)\n",
        "        if idx.sum() == 0:\n",
        "            recalls.append(1.0)  # neutral if a class is absent in this val fold/batch\n",
        "        else:\n",
        "            correct = (preds[idx] == targets[idx]).sum()\n",
        "            recalls.append(correct / idx.sum())\n",
        "    return np.array(recalls, dtype=np.float64)\n",
        "\n",
        "def gmean(recalls, eps=1e-8):\n",
        "    return float(np.exp(np.mean(np.log(recalls + eps))))\n"
      ],
      "metadata": {
        "id": "mj42zREUpTi9"
      },
      "id": "mj42zREUpTi9",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Data loaders\n",
        "# -----------------------------\n",
        "def build_loaders():\n",
        "    train_dir = Path(DRIVE_ROOT) / \"train\"\n",
        "    val_dir   = Path(DRIVE_ROOT) / \"val\"\n",
        "\n",
        "    train_recs = scan_split_3way(train_dir)\n",
        "    val_recs   = scan_split_3way(val_dir)\n",
        "\n",
        "    train_ds = PatchDataset(train_recs, transform=train_tfms)\n",
        "    val_ds   = PatchDataset(val_recs,   transform=eval_tfms)\n",
        "\n",
        "    train_sampler = make_weighted_sampler_3way(train_recs)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=BATCH_SIZE_PATCHES,\n",
        "      sampler=train_sampler,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=True,\n",
        "      collate_fn=collate_fn_max30,\n",
        "      drop_last=True,\n",
        "      persistent_workers=True,   # <â€” add\n",
        "      prefetch_factor=4          # <â€” add\n",
        "  )\n",
        "    val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=BATCH_SIZE_PATCHES,\n",
        "      shuffle=False,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=True,\n",
        "      collate_fn=collate_fn_max30,\n",
        "      drop_last=False,\n",
        "      persistent_workers=True,   # <â€” add\n",
        "      prefetch_factor=4          # <â€” add\n",
        "  )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "hogCHwgLpXjV"
      },
      "id": "hogCHwgLpXjV",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    print(\"--DATA LOADING--\")\n",
        "\n",
        "    train_loader, val_loader = build_loaders()\n",
        "\n",
        "    print(\"--DATA LOADING DONE--\")\n",
        "\n",
        "    model = EfficientNetB0_3Way(num_classes=3, dropout_p=DROPOUT_P).to(device)\n",
        "    # Channels-last speeds up memory access on NVIDIA GPUs\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    # Build discriminative optimizer groups (layer-wise LR)\n",
        "    optim_groups, init_lrs = build_discriminative_param_groups(\n",
        "        model, base_lr=BASE_LR, wd=WEIGHT_DECAY, mom=MOMENTUM, lr_div_factor=LR_DIV_FACTOR\n",
        "    )\n",
        "    optimizer = torch.optim.SGD(optim_groups)  # momentum/weight_decay are in groups\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    save_dir = \"./checkpoints\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(save_dir, \"efficientnet_b0_3way_best.pt\")\n",
        "\n",
        "    best_val_g = -1.0\n",
        "    warmup_steps = WARMUP_EPOCHS * STEPS_PER_EPOCH\n",
        "    global_step = 0\n",
        "\n",
        "    # Will hold counts from the *previous* epoch (printed at the start of the next)\n",
        "    prev_epoch_class_counts = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # ---------- Epoch header ----------\n",
        "        if prev_epoch_class_counts is None:\n",
        "            print(f\"\\nEpoch {epoch:02d} â€” first epoch: class counts will be shown from next epoch.\")\n",
        "        else:\n",
        "            b, a, m = prev_epoch_class_counts.tolist()\n",
        "            print(f\"\\nEpoch {epoch:02d} â€” last epoch class counts (patches seen): \"\n",
        "                  f\"Benign={b}, Atypical={a}, Malignant={m}\")\n",
        "\n",
        "        # ---------- Train ----------\n",
        "        model.train()\n",
        "        train_iter = iter(train_loader)\n",
        "        running_loss = 0.0\n",
        "        epoch_class_counts = torch.zeros(3, dtype=torch.long)  # counts of targets actually processed\n",
        "        epoch_patches = 0\n",
        "        t0 = time.time()\n",
        "\n",
        "        for step in range(STEPS_PER_EPOCH):\n",
        "            try:\n",
        "                images, targets, rois = next(train_iter)\n",
        "            except StopIteration:\n",
        "                train_iter = iter(train_loader)\n",
        "                images, targets, rois = next(train_iter)\n",
        "\n",
        "            # Send to GPU using channels_last\n",
        "            images  = images.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            # Track class counts & throughput\n",
        "            # (targets can be smaller than batch size due to per-ROI cap)\n",
        "            batch_counts = torch.bincount(targets, minlength=3).cpu()\n",
        "            epoch_class_counts += batch_counts\n",
        "            epoch_patches += int(targets.numel())\n",
        "\n",
        "            # Warm-up & post-warmup LR handling (per-group)\n",
        "            if global_step <= warmup_steps:\n",
        "                for pg, lr in zip(optimizer.param_groups, init_lrs):\n",
        "                    pg[\"lr\"] = lr\n",
        "            else:\n",
        "                for pg, lr in zip(optimizer.param_groups, init_lrs):\n",
        "                    pg[\"lr\"] = lr / FINETUNE_LR_DIVISOR\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=AMP):\n",
        "                logits, _ = model(images)\n",
        "                loss = ce(logits, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "        # End-of-epoch timing\n",
        "        epoch_time = time.time() - t0\n",
        "        patches_per_sec = epoch_patches / max(1e-6, epoch_time)\n",
        "\n",
        "        # ---------- Validate ----------\n",
        "        model.eval()\n",
        "        all_recalls = []\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n",
        "            for images, targets, rois in val_loader:\n",
        "                images  = images.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                targets = targets.to(device, non_blocking=True)\n",
        "                logits, _ = model(images)\n",
        "                preds = logits.argmax(1)\n",
        "                rec = per_class_recall(preds, targets, num_classes=3)\n",
        "                all_recalls.append(rec)\n",
        "\n",
        "        if all_recalls:\n",
        "            recalls = np.stack(all_recalls, axis=0).mean(axis=0)\n",
        "            val_g = gmean(recalls)\n",
        "        else:\n",
        "            recalls = np.zeros(3, dtype=np.float64)\n",
        "            val_g = 0.0\n",
        "\n",
        "        # Min/max LR snapshot across groups for visibility\n",
        "        cur_lrs = [pg[\"lr\"] for pg in optimizer.param_groups]\n",
        "        lr_min, lr_max = (min(cur_lrs), max(cur_lrs)) if len(cur_lrs) else (None, None)\n",
        "\n",
        "        # ---------- Log line (richer) ----------\n",
        "        print(f\"Epoch {epoch:02d} | train_loss={running_loss/STEPS_PER_EPOCH:.4f} \"\n",
        "              f\"| val_gmean={val_g:.4f} | recall(B,A,M)={np.round(recalls,3)}\")\n",
        "        print(f\"           | epoch_time={epoch_time:.1f}s | patches={epoch_patches} | throughput={patches_per_sec:.1f} patches/s \"\n",
        "              f\"| lr[min,max]=({lr_min:.5g}, {lr_max:.5g})\")\n",
        "        print(f\"           | train_class_counts (B,A,M) = {epoch_class_counts.tolist()}\")\n",
        "\n",
        "        # Save counts to show at *start* of next epoch\n",
        "        prev_epoch_class_counts = epoch_class_counts.clone()\n",
        "\n",
        "        # ---------- Save best ----------\n",
        "        if val_g > best_val_g:\n",
        "            best_val_g = val_g\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(),\n",
        "                \"class_names\": THREE_CLASS_NAMES,\n",
        "                \"val_gmean\": best_val_g,\n",
        "                \"image_size\": IMAGE_SIZE\n",
        "            }, ckpt_path)\n",
        "            print(f\"  â†³ saved best: {ckpt_path} (g-mean={best_val_g:.4f})\")\n",
        "\n",
        "    print(\"Done.\")\n",
        "    print(f\"Best val g-mean: {best_val_g:.4f}\")\n",
        "    print(f\"Checkpoint: {ckpt_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2QZ-MGZpa0a",
        "outputId": "9d242086-935f-4f53-851c-5882c04f6c05"
      },
      "id": "t2QZ-MGZpa0a",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--DATA LOADING--\n",
            "--DATA LOADING DONE--\n",
            "\n",
            "Epoch 01 â€” first epoch: class counts will be shown from next epoch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3629253794.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
            "/tmp/ipython-input-3629253794.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=AMP):\n",
            "/tmp/ipython-input-3629253794.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=1.1046 | val_gmean=0.7682 | recall(B,A,M)=[0.831 0.895 0.609]\n",
            "           | epoch_time=209.0s | patches=92160 | throughput=440.9 patches/s | lr[min,max]=(5.8594e-05, 0.03)\n",
            "           | train_class_counts (B,A,M) = [30938, 30453, 30769]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7682)\n",
            "\n",
            "Epoch 02 â€” last epoch class counts (patches seen): Benign=30938, Atypical=30453, Malignant=30769\n",
            "Epoch 02 | train_loss=1.1041 | val_gmean=0.7729 | recall(B,A,M)=[0.835 0.894 0.619]\n",
            "           | epoch_time=207.4s | patches=92160 | throughput=444.4 patches/s | lr[min,max]=(5.8594e-05, 0.03)\n",
            "           | train_class_counts (B,A,M) = [30979, 30451, 30730]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7729)\n",
            "\n",
            "Epoch 03 â€” last epoch class counts (patches seen): Benign=30979, Atypical=30451, Malignant=30730\n",
            "Epoch 03 | train_loss=1.1029 | val_gmean=0.7736 | recall(B,A,M)=[0.832 0.893 0.623]\n",
            "           | epoch_time=212.0s | patches=92160 | throughput=434.7 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30812, 30687, 30661]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7736)\n",
            "\n",
            "Epoch 04 â€” last epoch class counts (patches seen): Benign=30812, Atypical=30687, Malignant=30661\n",
            "Epoch 04 | train_loss=1.1029 | val_gmean=0.7692 | recall(B,A,M)=[0.832 0.894 0.612]\n",
            "           | epoch_time=209.6s | patches=92160 | throughput=439.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30808, 30458, 30894]\n",
            "\n",
            "Epoch 05 â€” last epoch class counts (patches seen): Benign=30808, Atypical=30458, Malignant=30894\n",
            "Epoch 05 | train_loss=1.1026 | val_gmean=0.7719 | recall(B,A,M)=[0.832 0.896 0.617]\n",
            "           | epoch_time=210.4s | patches=92160 | throughput=438.0 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30698, 30988, 30474]\n",
            "\n",
            "Epoch 06 â€” last epoch class counts (patches seen): Benign=30698, Atypical=30988, Malignant=30474\n",
            "Epoch 06 | train_loss=1.1026 | val_gmean=0.7717 | recall(B,A,M)=[0.833 0.896 0.616]\n",
            "           | epoch_time=211.5s | patches=92160 | throughput=435.8 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30770, 30747, 30643]\n",
            "\n",
            "Epoch 07 â€” last epoch class counts (patches seen): Benign=30770, Atypical=30747, Malignant=30643\n",
            "Epoch 07 | train_loss=1.1035 | val_gmean=0.7712 | recall(B,A,M)=[0.834 0.897 0.613]\n",
            "           | epoch_time=211.0s | patches=92160 | throughput=436.7 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30796, 30841, 30523]\n",
            "\n",
            "Epoch 08 â€” last epoch class counts (patches seen): Benign=30796, Atypical=30841, Malignant=30523\n",
            "Epoch 08 | train_loss=1.1026 | val_gmean=0.7748 | recall(B,A,M)=[0.836 0.894 0.622]\n",
            "           | epoch_time=209.9s | patches=92160 | throughput=439.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30675, 30808, 30677]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7748)\n",
            "\n",
            "Epoch 09 â€” last epoch class counts (patches seen): Benign=30675, Atypical=30808, Malignant=30677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "^^^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "^^    if w.is_alive():^\n",
            "\n",
            "  AssertionError :  can only test a child process \n",
            "  ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | train_loss=1.1015 | val_gmean=0.7743 | recall(B,A,M)=[0.836 0.896 0.62 ]\n",
            "           | epoch_time=211.0s | patches=92160 | throughput=436.7 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30774, 30521, 30865]\n",
            "\n",
            "Epoch 10 â€” last epoch class counts (patches seen): Benign=30774, Atypical=30521, Malignant=30865\n",
            "Epoch 10 | train_loss=1.1027 | val_gmean=0.7715 | recall(B,A,M)=[0.834 0.897 0.614]\n",
            "           | epoch_time=208.4s | patches=92160 | throughput=442.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30879, 30521, 30760]\n",
            "\n",
            "Epoch 11 â€” last epoch class counts (patches seen): Benign=30879, Atypical=30521, Malignant=30760\n",
            "Epoch 11 | train_loss=1.1014 | val_gmean=0.7682 | recall(B,A,M)=[0.834 0.895 0.607]\n",
            "           | epoch_time=202.4s | patches=92160 | throughput=455.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30748, 30609, 30803]\n",
            "\n",
            "Epoch 12 â€” last epoch class counts (patches seen): Benign=30748, Atypical=30609, Malignant=30803\n",
            "Epoch 12 | train_loss=1.1013 | val_gmean=0.7677 | recall(B,A,M)=[0.832 0.897 0.606]\n",
            "           | epoch_time=199.8s | patches=92160 | throughput=461.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30578, 30656, 30926]\n",
            "\n",
            "Epoch 13 â€” last epoch class counts (patches seen): Benign=30578, Atypical=30656, Malignant=30926\n",
            "Epoch 13 | train_loss=1.1019 | val_gmean=0.7683 | recall(B,A,M)=[0.834 0.897 0.606]\n",
            "           | epoch_time=202.0s | patches=92160 | throughput=456.1 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30666, 30739, 30755]\n",
            "\n",
            "Epoch 14 â€” last epoch class counts (patches seen): Benign=30666, Atypical=30739, Malignant=30755\n",
            "Epoch 14 | train_loss=1.1008 | val_gmean=0.7723 | recall(B,A,M)=[0.835 0.898 0.614]\n",
            "           | epoch_time=202.1s | patches=92160 | throughput=456.0 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30653, 30601, 30906]\n",
            "\n",
            "Epoch 15 â€” last epoch class counts (patches seen): Benign=30653, Atypical=30601, Malignant=30906\n",
            "Epoch 15 | train_loss=1.1018 | val_gmean=0.7699 | recall(B,A,M)=[0.833 0.895 0.612]\n",
            "           | epoch_time=202.3s | patches=92160 | throughput=455.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30724, 30617, 30819]\n",
            "\n",
            "Epoch 16 â€” last epoch class counts (patches seen): Benign=30724, Atypical=30617, Malignant=30819\n",
            "Epoch 16 | train_loss=1.1020 | val_gmean=0.7688 | recall(B,A,M)=[0.835 0.897 0.607]\n",
            "           | epoch_time=200.1s | patches=92160 | throughput=460.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30604, 30892, 30664]\n",
            "\n",
            "Epoch 17 â€” last epoch class counts (patches seen): Benign=30604, Atypical=30892, Malignant=30664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive(): \n",
            "      ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f9b97fe5300>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | train_loss=1.1022 | val_gmean=0.7750 | recall(B,A,M)=[0.83  0.896 0.626]\n",
            "           | epoch_time=201.4s | patches=92160 | throughput=457.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30754, 30917, 30489]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7750)\n",
            "\n",
            "Epoch 18 â€” last epoch class counts (patches seen): Benign=30754, Atypical=30917, Malignant=30489\n",
            "Epoch 18 | train_loss=1.1015 | val_gmean=0.7722 | recall(B,A,M)=[0.834 0.896 0.616]\n",
            "           | epoch_time=203.5s | patches=92160 | throughput=452.8 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30799, 30583, 30778]\n",
            "\n",
            "Epoch 19 â€” last epoch class counts (patches seen): Benign=30799, Atypical=30583, Malignant=30778\n",
            "Epoch 19 | train_loss=1.1023 | val_gmean=0.7716 | recall(B,A,M)=[0.833 0.894 0.617]\n",
            "           | epoch_time=204.2s | patches=92160 | throughput=451.3 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30855, 30698, 30607]\n",
            "\n",
            "Epoch 20 â€” last epoch class counts (patches seen): Benign=30855, Atypical=30698, Malignant=30607\n",
            "Epoch 20 | train_loss=1.1008 | val_gmean=0.7710 | recall(B,A,M)=[0.836 0.896 0.612]\n",
            "           | epoch_time=202.8s | patches=92160 | throughput=454.4 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30902, 30836, 30422]\n",
            "\n",
            "Epoch 21 â€” last epoch class counts (patches seen): Benign=30902, Atypical=30836, Malignant=30422\n",
            "Epoch 21 | train_loss=1.1008 | val_gmean=0.7719 | recall(B,A,M)=[0.833 0.897 0.615]\n",
            "           | epoch_time=202.3s | patches=92160 | throughput=455.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30583, 30771, 30806]\n",
            "\n",
            "Epoch 22 â€” last epoch class counts (patches seen): Benign=30583, Atypical=30771, Malignant=30806\n",
            "Epoch 22 | train_loss=1.1005 | val_gmean=0.7722 | recall(B,A,M)=[0.835 0.898 0.615]\n",
            "           | epoch_time=202.6s | patches=92160 | throughput=455.0 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30600, 30760, 30800]\n",
            "\n",
            "Epoch 23 â€” last epoch class counts (patches seen): Benign=30600, Atypical=30760, Malignant=30800\n",
            "Epoch 23 | train_loss=1.0997 | val_gmean=0.7699 | recall(B,A,M)=[0.833 0.9   0.609]\n",
            "           | epoch_time=198.0s | patches=92160 | throughput=465.4 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30454, 30827, 30879]\n",
            "\n",
            "Epoch 24 â€” last epoch class counts (patches seen): Benign=30454, Atypical=30827, Malignant=30879\n",
            "Epoch 24 | train_loss=1.1008 | val_gmean=0.7713 | recall(B,A,M)=[0.834 0.897 0.614]\n",
            "           | epoch_time=198.7s | patches=92160 | throughput=463.9 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30446, 30867, 30847]\n",
            "\n",
            "Epoch 25 â€” last epoch class counts (patches seen): Benign=30446, Atypical=30867, Malignant=30847\n",
            "Epoch 25 | train_loss=1.1018 | val_gmean=0.7691 | recall(B,A,M)=[0.833 0.897 0.609]\n",
            "           | epoch_time=198.8s | patches=92160 | throughput=463.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30718, 30890, 30552]\n",
            "\n",
            "Epoch 26 â€” last epoch class counts (patches seen): Benign=30718, Atypical=30890, Malignant=30552\n",
            "Epoch 26 | train_loss=1.1016 | val_gmean=0.7742 | recall(B,A,M)=[0.83  0.896 0.624]\n",
            "           | epoch_time=199.3s | patches=92160 | throughput=462.5 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30777, 30605, 30778]\n",
            "\n",
            "Epoch 27 â€” last epoch class counts (patches seen): Benign=30777, Atypical=30605, Malignant=30778\n",
            "Epoch 27 | train_loss=1.1014 | val_gmean=0.7732 | recall(B,A,M)=[0.834 0.897 0.618]\n",
            "           | epoch_time=198.6s | patches=92160 | throughput=464.1 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30648, 30603, 30909]\n",
            "\n",
            "Epoch 28 â€” last epoch class counts (patches seen): Benign=30648, Atypical=30603, Malignant=30909\n",
            "Epoch 28 | train_loss=1.1015 | val_gmean=0.7746 | recall(B,A,M)=[0.834 0.898 0.621]\n",
            "           | epoch_time=199.9s | patches=92160 | throughput=461.1 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30736, 30647, 30777]\n",
            "\n",
            "Epoch 29 â€” last epoch class counts (patches seen): Benign=30736, Atypical=30647, Malignant=30777\n",
            "Epoch 29 | train_loss=1.1022 | val_gmean=0.7709 | recall(B,A,M)=[0.832 0.898 0.613]\n",
            "           | epoch_time=201.2s | patches=92160 | throughput=458.1 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30665, 30798, 30697]\n",
            "\n",
            "Epoch 30 â€” last epoch class counts (patches seen): Benign=30665, Atypical=30798, Malignant=30697\n",
            "Epoch 30 | train_loss=1.1016 | val_gmean=0.7700 | recall(B,A,M)=[0.833 0.897 0.611]\n",
            "           | epoch_time=201.8s | patches=92160 | throughput=456.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30661, 30807, 30692]\n",
            "\n",
            "Epoch 31 â€” last epoch class counts (patches seen): Benign=30661, Atypical=30807, Malignant=30692\n",
            "Epoch 31 | train_loss=1.1002 | val_gmean=0.7692 | recall(B,A,M)=[0.832 0.897 0.609]\n",
            "           | epoch_time=200.6s | patches=92160 | throughput=459.4 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30833, 30694, 30633]\n",
            "\n",
            "Epoch 32 â€” last epoch class counts (patches seen): Benign=30833, Atypical=30694, Malignant=30633\n",
            "Epoch 32 | train_loss=1.1000 | val_gmean=0.7704 | recall(B,A,M)=[0.833 0.896 0.613]\n",
            "           | epoch_time=200.6s | patches=92160 | throughput=459.4 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30790, 30686, 30684]\n",
            "\n",
            "Epoch 33 â€” last epoch class counts (patches seen): Benign=30790, Atypical=30686, Malignant=30684\n",
            "Epoch 33 | train_loss=1.1003 | val_gmean=0.7732 | recall(B,A,M)=[0.833 0.898 0.618]\n",
            "           | epoch_time=200.5s | patches=92160 | throughput=459.6 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30806, 30819, 30535]\n",
            "\n",
            "Epoch 34 â€” last epoch class counts (patches seen): Benign=30806, Atypical=30819, Malignant=30535\n",
            "Epoch 34 | train_loss=1.1000 | val_gmean=0.7761 | recall(B,A,M)=[0.835 0.897 0.624]\n",
            "           | epoch_time=200.7s | patches=92160 | throughput=459.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30650, 30664, 30846]\n",
            "  â†³ saved best: ./checkpoints/efficientnet_b0_3way_best.pt (g-mean=0.7761)\n",
            "\n",
            "Epoch 35 â€” last epoch class counts (patches seen): Benign=30650, Atypical=30664, Malignant=30846\n",
            "Epoch 35 | train_loss=1.1013 | val_gmean=0.7698 | recall(B,A,M)=[0.834 0.896 0.61 ]\n",
            "           | epoch_time=200.7s | patches=92160 | throughput=459.2 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30971, 30514, 30675]\n",
            "\n",
            "Epoch 36 â€” last epoch class counts (patches seen): Benign=30971, Atypical=30514, Malignant=30675\n",
            "Epoch 36 | train_loss=1.1023 | val_gmean=0.7718 | recall(B,A,M)=[0.833 0.899 0.614]\n",
            "           | epoch_time=201.4s | patches=92160 | throughput=457.7 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30609, 30691, 30860]\n",
            "\n",
            "Epoch 37 â€” last epoch class counts (patches seen): Benign=30609, Atypical=30691, Malignant=30860\n",
            "Epoch 37 | train_loss=1.1000 | val_gmean=0.7700 | recall(B,A,M)=[0.836 0.897 0.609]\n",
            "           | epoch_time=201.8s | patches=92160 | throughput=456.7 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30622, 30786, 30752]\n",
            "\n",
            "Epoch 38 â€” last epoch class counts (patches seen): Benign=30622, Atypical=30786, Malignant=30752\n",
            "Epoch 38 | train_loss=1.1009 | val_gmean=0.7702 | recall(B,A,M)=[0.835 0.895 0.612]\n",
            "           | epoch_time=201.7s | patches=92160 | throughput=457.0 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30643, 30605, 30912]\n",
            "\n",
            "Epoch 39 â€” last epoch class counts (patches seen): Benign=30643, Atypical=30605, Malignant=30912\n",
            "Epoch 39 | train_loss=1.1017 | val_gmean=0.7716 | recall(B,A,M)=[0.834 0.897 0.614]\n",
            "           | epoch_time=200.6s | patches=92160 | throughput=459.3 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30438, 30841, 30881]\n",
            "\n",
            "Epoch 40 â€” last epoch class counts (patches seen): Benign=30438, Atypical=30841, Malignant=30881\n",
            "Epoch 40 | train_loss=1.1005 | val_gmean=0.7723 | recall(B,A,M)=[0.834 0.896 0.616]\n",
            "           | epoch_time=200.1s | patches=92160 | throughput=460.5 patches/s | lr[min,max]=(5.8594e-06, 0.003)\n",
            "           | train_class_counts (B,A,M) = [30620, 30772, 30768]\n",
            "Done.\n",
            "Best val g-mean: 0.7761\n",
            "Checkpoint: ./checkpoints/efficientnet_b0_3way_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xL52EBtHZXdY"
      },
      "id": "xL52EBtHZXdY"
    },
    {
      "cell_type": "code",
      "source": [
        "SEVEN_TO_THREE = {\n",
        "    \"0_N\":0, \"1_PB\":0, \"2_UDH\":0, \"3_FEA\":1, \"4_ADH\":1, \"5_DCIS\":2, \"6_IC\":2,\n",
        "}\n",
        "# Sanity:\n",
        "root = Path(DRIVE_ROOT) / \"train\"\n",
        "actual = sorted([d.name for d in root.iterdir() if d.is_dir()])\n",
        "print(\"[FOLDERS]:\", actual)\n",
        "missing = [d for d in actual if d not in SEVEN_TO_THREE]\n",
        "print(\"[UNMAPPED]:\", missing)   # must be []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWYuJqyfPZVS",
        "outputId": "3662f6f2-146c-4248-f833-c8e67816ba48"
      },
      "id": "fWYuJqyfPZVS",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDERS]: ['0_N', '1_PB', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n",
            "[UNMAPPED]: []\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}